def data_read(dir):

def word_2_index(all_data):

def word_onehot(word_2_index):

def MyDataset():
	def __init__(self,all_data,all_lable):
	
	def __gititem__(self,index)

	def __len__(self)


class MyModel(nn.model):
	def__init__(self)
	super__init__()
	self.w=nn.Linear()
	self.loss_fun=nn.CrossEntropyLoss()

	def forward(x,lable=None)

if __name__ == "__main__":
    all_data,all_lable=data_read(os.path.join("data","文本分类","train.txt"),20000)
    test_data, test_lable = data_read(os.path.join("data", "文本分类", "train.txt"),-300)
    word_2_index, index_2_word = word_2_index(all_data)
    word_onehot = make_word_onehot(word_2_index)

    lr = 0.01
    batch_size = 1
    epoch = 100
    max_len = 30

    device="cuda" if torch.cuda.is_available() else "cpu"
    train_dataset = MyDataset(all_data, all_lable)
    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)
    test_dataset = MyDataset(test_data, test_lable)
    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    model = MyModel()
    opt = torch.optim.Adam(model.parameters(), lr=lr)

    for e in range(epoch):
        for batch_text, batch_label in tqdm(train_dataloader):
            batch_text.to(device)
            batch_label.to(device)
            loss=model.forward(batch_text, batch_label)
            loss.backward()
            opt.step()
            opt.zero_grad()

            right_num=0
        for batch_text, batch_lable in train_dataloader:
            batch_text.to(device)
            batch_label.to(device)
            pre= model.forward(batch_text)

            right_num += int(torch.sum(pre==batch_lable))

        acc=right_num/len(test_dataset)
        print(f"epoch:{e},acc;{acc:.3f}")

#非pytorch版：word和lable都要进行onehot编码，手写前向传播、反向传播

#pytorch版：lable不需要进行onehot编码，一键前向传播、反向传播


